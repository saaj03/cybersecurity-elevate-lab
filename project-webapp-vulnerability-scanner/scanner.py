import requests, re
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# Some test payloads
xss_payloads = ["<script>alert(1)</script>", "'\"><img src=x onerror=alert(1)>"]
sqli_payloads = ["' OR '1'='1", '" OR "1"="1']

def crawl(url, max_pages=5):
    """Very small crawler"""
    seen, to_visit, pages = set(), [url], []
    while to_visit and len(pages) < max_pages:
        u = to_visit.pop()
        if u in seen: continue
        seen.add(u)
        try:
            r = requests.get(u, timeout=5)
        except:
            continue
        if "text/html" not in r.headers.get("Content-Type",""):
            continue
        pages.append(u)
        soup = BeautifulSoup(r.text, "html.parser")
        for a in soup.find_all("a", href=True):
            link = urljoin(u, a["href"])
            if link.startswith(url):
                to_visit.append(link)
    return pages

def test_xss(url):
    issues = []
    for p in xss_payloads:
        test = url + f"?q={p}"
        try:
            r = requests.get(test, timeout=5)
            if p in r.text:
                issues.append(("XSS", test, p))
        except: pass
    return issues

def test_sqli(url):
    issues = []
    for p in sqli_payloads:
        test = url + f"?id={p}"
        try:
            r = requests.get(test, timeout=5)
            if re.search("sql|error|mysql|syntax", r.text, re.I):
                issues.append(("SQLi", test, p))
        except: pass
    return issues

def scan(target):
    pages = crawl(target)
    findings = []
    for p in pages:
        findings += test_xss(p)
        findings += test_sqli(p)
        findings += check_headers(p)
        findings += check_open_redirect(p)
        findings += check_csrf(p)
    return findings

def check_headers(url):
    issues = []
    try:
        r = requests.get(url, timeout=5)
        headers = r.headers

        if "X-Frame-Options" not in headers:
            issues.append(("Missing Security Header", url, "X-Frame-Options"))

        if "Content-Security-Policy" not in headers:
            issues.append(("Missing Security Header", url, "Content-Security-Policy"))
    except:
        pass
    return issues

def check_open_redirect(url):
    issues = []
    payload = "http://evil.com"
    if "redirect" in url or "next" in url or "url=" in url:
        test = url + payload
        try:
            r = requests.get(test, timeout=5, allow_redirects=False)
            if "evil.com" in r.headers.get("Location", ""):
                issues.append(("Open Redirect", test, payload))
        except:
            pass
    return issues

def check_csrf(url):
    issues = []
    try:
        r = requests.get(url, timeout=5)
        soup = BeautifulSoup(r.text, "html.parser")
        forms = soup.find_all("form")
        for form in forms:
            if not form.find("input", {"name": re.compile("csrf", re.I)}):
                issues.append(("Possible CSRF", url, "Form without CSRF token"))
    except:
        pass
    return issues


if __name__ == "__main__":
    target = input("Enter target URL: ")
    results = scan(target)
    for r in results:
        print("[!] Found", r)

 # Save reports
    import reporter
    reporter.save_json(results)
    reporter.save_csv(results)
    reporter.save_html(results)
    print("\nReports saved as report.json, report.csv, report.html")

